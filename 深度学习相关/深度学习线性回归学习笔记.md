# 深度学习线性回归学习笔记

## 线性回归

### 一个简化模型

- 假设1：影响房价的关键因素是卧室个数，卫生间个数和居住面积，记为x1,x2,x3。
- 假设2：成交价是关键因素的加权和。
  - y = w1*x1+w2*x2+w3*x3+b

**线性模型**

- 给定了n维输入 **x** = [x1,x2,…,xn]T
- 线性模型有一个n维权重和一个标量偏差
  - **w** = [w1,w2,…,wn]T
- 输出是输入的加权和
  - y = y = w1*x1+w2*x2+…+wn*xn+b
- 向量版本: y = <w, x> + b

这个简单模型可以看作是一个简单的单层神经网络。神经网络来源于神经科学

### 衡量预估质量

- 比较真实值和预估值，比如房屋售价和估价
- 假设y是真实值，y^y^是估计值，我们可以比较

![20230323142018](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323142018.jpg)

这个叫做平方损失

**训练数据**

- 收集一些数据来决定参数值（权重和偏差），例如过去六个月买的房子
- 这些数据被称为训练数据，通常越多越好
- 假设我们有n个样本，记
  - **x** = [x1,x2,…,xn]T **y** = [w1,w2,…,wn]T

**参数学习**

- 训练损失
  - ![20230323142439](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323142439.jpg)
- 最小化损失来学习参数
  - ![20230323142540](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323142540.jpg)

**显示解**

![20230323143030](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323143030.jpg)

#### 总结

- 线性回归是对n维输入的加权，外加偏差
- 使用平方损失来衡量预测值和真实值的差异
- 线性回归有显示解
- 线性回归可以看作是单层神经网络

## 基础优化方法

### 梯度下降

- 挑选一个初始值w0
- 重复迭代参数 t = 1, 2, 3
  - wt = wt-1 - �η$$\frac {\partial(l)}{\partial(w_{t-1})}$$
  - 沿梯度方向将增加损失函数值
  - 学习率：步长的**超参数**(一个需要认为来指定的值)
  - �η$$\frac {\partial(l)}{\partial(w_{t-1})}$$：下降的有多远
  - ![20230323144930](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323144930.jpg)

学习率的不能太大也不能太小

![20230323145039](https://typorazkr------------image.oss-cn-beijing.aliyuncs.com/test/20230323145039.jpg)

学习率太小会多次计算浪费资源 且容易陷入局部最小值，学习率太大会来回振荡做不到下降的结果

### 小批量随机梯度下降

- 在整个训练集上计算梯度太昂贵

  - 一个深度神经网络模型可能需要数分钟甚至数个小时
  - 我们可以随机采样b个样本i1,i2…ib

  来近似损失

  ![img](http://tiebapic.baidu.com/forum/w%3D580/sign=6c611d452981800a6ee58906813433d6/b55ab71c8701a18b3aeff685db2f07082938fe07.jpg?tbpicau=2023-04-08-05_5158e26188696a8268beec3e51ec8813)

  

  - b是批量大小，另外一个重要的超参数

**选择批量大小**

- 不能太小：每次计算量太小，不适合并行来最大利用计算资源
- 不能太大：内存消耗增加，浪费计算，例如如果所有样本都是相同的

#### 总结

- 梯度下降通过不断沿着反梯度方向更新参数求解
- 小批量随机梯度下降是深度学习默认的求解算法
- 两个重要的超参数是批量大小和学习率

## [线性回归的从零开始实现](https://zh-v2.d2l.ai/chapter_linear-networks/linear-regression-scratch.html)

```
%matplotlib inline
import random
import torch
from d2l import torch as d2l
```

### 生成数据集

```
def synthetic_data(w, b, num_examples):  #@save
    """生成y=Xw+b+噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
```

注意，`features`中的每一行都包含一个二维数据样本， `labels`中的每一行都包含一维标签值（一个标量）。

```
d2l.set_figsize()
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1);
```

![../_images/output_linear-regression-scratch_58de05_51_0.svg](https://zh-v2.d2l.ai/_images/output_linear-regression-scratch_58de05_51_0.svg)

## 读取数据集

```
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    # 这些样本是随机读取的，没有特定的顺序
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]
```

## 初始化模型参数

```
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。

## 定义模型

接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。 回想一下，要计算线性模型的输出， 我们只需计算输入特征X和模型权重w的矩阵-向量乘法后加上偏置b。 注意，上面的Xw是一个向量，而b是一个标量

```
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

##  定义优化算法

```
def sgd(params, lr, batch_size):  #@save
    """小批量随机梯度下降"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
```

## 训练

```
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # X和y的小批量损失
        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，
        # 并以此计算关于[w,b]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

```
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')
```

```
w的估计误差: tensor([ 0.0003, -0.0002], grad_fn=<SubBackward0>)
b的估计误差: tensor([0.0010], grad_fn=<RsubBackward1>)
```

## 线性回归的简洁实现

通过深度学习框架来简洁实现线性回归模型生成数据集

```
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

**调用框架中现有的api来读取数据**

```
def load_array(data_arrays, batch_size, is_train=True):
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)
batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

**使用框架的预定义好的层**

```
from torch import nn

net = nn.Sequential(nn.Linear(2,1))
```

**初始化模型参数**

```
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

计算均方误差使用的是`MSELoss`类，也称为平方范数

```
loss = nn.MSELoss()
```

------

```
trainer = torch.optim.SGD(net.parameters(), lr=0.03) #实例化SGD实例
```

**训练过程**

```
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

## Softmax回归

1. 回归估计一个连续值
2. 分类预测一个离散类别

MNIST:手写数字识别 ImageNet:自然物体分类

**从回归到多类分类**

回归

- 单连续值输出
- 自然区间R
- 跟真实值的u区别作为损失

分类

- 通常是多个输出
- 输出i是预测为第i类的置信度

**从回归到多类分类-均方损失**

- 对类别进行一位有效编码
  - y = [y1,…yn]T
  - if i = y:yi=1
  - else:yi = 0
- 使用均方损失训练
- 最大值为预测
  - y^y^=argmaxoi

**从回归到多类分类–无校验比例**

- 对类别进行一位有效编码

- 最大值为预测：y^y^=argmaxoi

- 需要更置信的识别正确类（大余量）

  - $$
    Oy-Oi>=ΔΔ(y,i)
    $$

    

**从回归到多类分类–校验比例**

- 输出匹配概率(非负，和为1)

  

- 

$$
\hat{y_i}=\frac{exp(O_i)}{\sum_{k}{exp(O_k)}}
$$

**Softmax和交叉熵损失**

- 交叉熵常用来衡量两个概率的区别：H(p,q) = $$\sum\limits_{i}-p_ilog(q_i)$$

- 将其作为损失：

  - $$
    H(p,q) = \sum\limits_{i}-p_ilog(q_i)
    $$

    

- 其梯度是真实概率和预测概率的区别

  - $$
     \partial o_il(y,\hat{y})=softmax(o)_i-y_i
    $$

    ## 图像分类数据集

    MNIST数据集是图像分类中最广泛使用的数据集之一，但作为基准数据过于简单，所以我们将使用类似但是复杂的fashion-MNIST数据集

    ```
    %matplotlib inline
    import torch
    import torchvision
    from torch.utils import data
    from torchvision import transforms
    from d2l import torch as d2l
    
    d2l.use_svg_display()
    ```

    通过框架中的内置函数将Fashion—MINIST数据集下载并读取到内存中

    ```
    # 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
    # 并除以255使得所有像素的数值均在0～1之间
    trans = transforms.ToTensor()
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    mnist_train[0][0].shape # 第一张图片的形状
    # 测试数据集不参与训练，只是用来评判我们模型的好坏
    ```

    两个可视化数据集的函数

    ```
    def get_fashion_mnist_labels(labels):  #@save
        """返回Fashion-MNIST数据集的文本标签"""
        text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                       'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
        return [text_labels[int(i)] for i in labels]
    def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
        """绘制图像列表"""
        figsize = (num_cols * scale, num_rows * scale)
        _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
        axes = axes.flatten()
        for i, (ax, img) in enumerate(zip(axes, imgs)):
            if torch.is_tensor(img):
                # 图片张量
                ax.imshow(img.numpy())
            else:
                # PIL图片
                ax.imshow(img)
            ax.axes.get_xaxis().set_visible(False)
            ax.axes.get_yaxis().set_visible(False)
            if titles:
                ax.set_title(titles[i])
        return axes
    ```

    几个样本的图像及其相应的标签

    ```
    X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
    show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
    ```

    读取小批量数据

    ```
    batch_size = 256
    
    def get_dataloader_workers():  #@save
        """使用4个进程来读取数据"""
        return 4
    
    train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                                 num_workers=get_dataloader_workers())
    
    timer = d2l.Timer()
    for X, y in train_iter:
        continue
    f'{timer.stop():.2f} sec'
    ```

    ## Softmax回归的从零实现

    ```
    import torch
    from IPython import display
    from d2l import torch as d2l
    batch_size = 256
    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
    ```

    **初始化参数模型**

    战平每个图象，将它们视为长度为784的向量。因为我们的数据集有10个类别，所以网络输出维度为10

    ```
    num_inputs = 784
    num_outputs = 10
    
    W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
    b = torch.zeros(num_outputs, requires_grad=True)
    ```

    给定一个矩阵X，我们对所有元素进行求和

    ```
    X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    X.sum(0, keepdim=True), X.sum(1, keepdim=True)
    ```

    实现Softmax

    ```
    def softmax(X):
        X_exp = torch.exp(X)
        partition = X_exp.sum(1, keepdim=True)
        return X_exp / partition  # 这里应用了广播机制
    ```

    我们将每一个元素变成一个非负数。此外依据概率原理，每行的总和为1

    ```
    X = torch.normal(0,1,(2,5))
    x_prob = softmax(X)
    X_prob, X_prob.sum(1)
    ```

    实现softmax回归模型

    ```
    def net(x):
        return softmax(torch.matmul(x.reshape((-1, W.shape[0])), W) + b)
    ```

    创建一个数据`y_hat`,其中包含2个样本在3个类别的预测概率，使用`y`作为`y_hat`中概率的索引

    ```
    y = torch.tensor([0, 2])
    y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
    y_hat([0.1000, 0.5000])
    ```

    实现交叉熵损失函数

    ```
    def cross_entropy(y_hat, y):
        return -torch.log(y_hat[range(len(len(y_hat))), y])
    cross_entropy(y_hat, y)
    ```

    将预测类别与真实y元素的比较：

    ```
    def accuracy(y_hat, y):
        """计算预测正确的数量"""
        if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
            y_hat = y_hat.argmax(axis=1)
        cmp = y_hat.type(y.dtype) == y
        return float(cmp.type(y.dtype).sum())
    
    accuracy(y_hat, y) / len(y)
    ```

    我们可以评估在任意模型`net`的准确率

    ```
    def evaluate_accuracy(net, data_iter):  #@save
        """计算在指定数据集上模型的精度"""
        if isinstance(net, torch.nn.Module):
            net.eval()  # 将模型设置为评估模式
        metric = Accumulator(2)  # 正确预测数、预测总数
        with torch.no_grad():
            for X, y in data_iter:
                metric.add(accuracy(net(X), y), y.numel())
        return metric[0] / metric[1] # 分类正确的样本数和总样本数的商
    ```

    Accumulator实例中创建了2个变量，分别用于存储正确预测的数量和预测的总数量

    ```
    class Accumulator:  #@save
        """在n个变量上累加"""
        def __init__(self, n):
            self.data = [0.0] * n
    
        def add(self, *args):
            self.data = [a + float(b) for a, b in zip(self.data, args)]
    
        def reset(self):
            self.data = [0.0] * len(self.data)
    
        def __getitem__(self, idx):
            return self.data[idx]
       
    evaluate_accuracy(net, test_iter)
    ```

    Softmax回归的训练

    ```
    def train_epoch_ch3(net, train_iter, loss, updater):  #@save
        if isinstance(net, torch.nn.Module):
            net.train()
        metric = Accumulator(3)
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y)
            if isinstance(updater, torch.optim.Optimizer):
                updater.zero_grad()
                l.mean().backward()
                updater.step()
                metric.add(
                    float(l) * len(y), accuracy(y_hat, y),
                    y.size().numel()
                )
            else:
                l.sum().backward()
                updater(X.shape[0])
            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
        return metric[0] / metric[2], metric[1] / metric[2]
    ```

    训练函数

    ```
    def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
        """训练模型（定义见第3章）"""
        animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                            legend=['train loss', 'train acc', 'test acc'])
        for epoch in range(num_epochs):
            train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
            test_acc = evaluate_accuracy(net, test_iter)
            animator.add(epoch + 1, train_metrics + (test_acc,))
        train_loss, train_acc = train_metrics
        assert train_loss < 0.5, train_loss
        assert train_acc <= 1 and train_acc > 0.7, train_acc
        assert test_acc <= 1 and test_acc > 0.7, test_acc
    ```

    小批量随机梯度下降来优化模型的损失函数

    ```
    lr = 0.1
    
    def updater(batch_size):
        return d2l.sgd([W, b], lr, batch_size)
    ```

    训练模型10个迭代周期

    ```
    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
    ```

    预测

    ```
    def predict_ch3(net, test_iter, n=6):  #@save
        """预测标签（定义见第3章）"""
        for X, y in test_iter:
            break
        trues = d2l.get_fashion_mnist_labels(y)
        preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
        titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
        d2l.show_images(
            X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])
    
    predict_ch3(net, test_iter)
    ```

    ## Softmax回归的简洁实现

    Softmax回归的输出层是一个全连接层

    ```
    # Pytorch不会隐式地调整输入地形状
    # 因此，我们定义了展平层在线性层前调整网络输入的形状
    net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))
    
    def init_weights(m):
        if type(m) == nn.Linear:
            nn.init.normal_(m.weight, std=0.01)
            
    net.apply(init_weights);
    ```

    在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数

    ```
    loss = nn.CrossEntropyLoss()
    ```

    使用学习率为0.1的小批量随机梯度下降作为优化算法

    ```
    trainer = torch.optim.SGD(net.parameters(), lr=0.1)
    ```

    调用第三章定义的训练函数来训练模型

    ```
    num_epochs = 10
    d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
    ```