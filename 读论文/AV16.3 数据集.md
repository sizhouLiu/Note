# AV16.3 数据集



摘要。在少量的短示例中评估演讲者定位或跟踪算法的质量是困难的，特别是当地面真相不存在或不明确定时。朝着系统性评估这类算法性能的一步是在一系列真实录音上提供时间连续的演讲者位置注释，涵盖各种测试情况。感兴趣的领域包括音频、视频和音频-视觉演讲者定位和跟踪。所需的位置注释可以是2D（图像平面）或3D（物理空间）。本文介绍和描述了一个名为“AV16.3”的音频-视觉数据语料库，以及基于校准摄像机的3D位置注释方法。“16.3”代表在一个会议室中以全面同步方式记录的16个麦克风和3台摄像机。该语料库的一部分已被成功用于报告研究结果。



1 介绍
本文描述了一个名为“AV16.3”的音视频数据语料库，记录在会议室环境中。“16.3”代表了16个麦克风和3个摄像头，以完全同步的方式录制。中心思想是使用校准的摄像头为测试音频定位和跟踪算法提供连续的三维（3-D）说话者位置注释。特别关注同时说话时的重叠，即当多个说话者同时讲话时。重叠在多方言自发性语音中是一个重要问题，比如在会议中发现[9]。

由于有视觉记录可用，可以测试视频和音频-视觉跟踪算法。因此，我们定义并录制了一系列场景，以涵盖各种研究领域，包括音频、视频和音频-视觉在会议室中进行人员定位和跟踪。可能的应用范围从会议的自动分析到强大的语音获取和视频监控等。为了允许如此广泛的研究主题，这里广泛定义了“会议室环境”，包括各种情况，从大多数时间说话者坐着的“会议情况”到大多数时间说话者在移动中的“动态情况”。这与现有的相关数据库有所不同：例如ICSI数据库[4]包含自然会议的仅音频记录，CUAVE数据库[7]包含音频-视觉记录（特写镜头），但侧重于多模态语音识别，CIPIC [1]数据库侧重于头部相关转移功能。我们选择将整个数据库集中在一个研究主题上，而是选择一个单一的、通用的设置，为不同的录音提供非常不同的情景。我们的目标是提供注释，无论是在麦克风阵列的参考中的“真实”3-D说话者位置，还是在每个摄像头的图像平面中的“真实”2-D头部/脸部位置。这种注释允许对定位和跟踪算法进行系统性评估，而不是在一些没有注释的短片上进行主观评估。

据我们所知，目前还没有这样的音频-视频数据库公开可用。我们在这里呈现的数据集已经开始被使用：两个具有静态说话者的录音已经成功用于报告真实多源语音录音的结果[5]。在调查现有的演讲者位置注释解决方案时，我们发现了各种解决方案，每个人都需要佩戴设备，以及一个定位每个个人设备的基础设备。然而，这些解决方案要么成本高昂且性能极佳（高精度和采样率，基础设备和个人设备之间没有连接），要么成本低廉但精度较低和/或有较高的限制（例如，个人设备连接到基础设备）。因此，我们选择使用校准的摄像头重构说话者的3-D位置。重要的是要注意，这种解决方案可能是非侵入性的，事实上，在这里呈现的部分语料库中没有演员佩戴任何特殊标记。在语料库的设计中，需要满足两个相互矛盾的约束:1)说话者占用的区域应该足够大，以涵盖“会议情况”和“动态情况”2）所有摄像头应该完全可见该区域。这允许对摄像头位置进行系统优化。同时带来了3.D位置信息的强健重建，因为可以使用所有摄像头的信息。本文的其余部分组织如下：第2节描述了用于提供3-D嘴巴位置注释的物理设置和摄像头校准过程，第3节描述并说明了一组通过互联网公开可用的序列，第4节讨论了注释协议，并报告了目前注释工作的当前状态。

2 物理设置和摄像机校准

对于可能的发言者位置，我们选择了会议室中桌子周围的L形区域，如图1所示。有关会议室的一般描述可以在[6]中找到。L形区域是一个3米长、2米宽的矩形，减去了桌子占据的0.6米宽的部分。这个选择是为了满足引言中提到的两个约束的折衷。使用不同摄像头拍摄的视图可以在图2中看到。数据本身在第3节中描述。

硬件选择及其动机在第2.1节中描述。我们采用了一个两步策略来放置摄像头并对其进行校准。首先，通过包括子优化摄像头校准在内的循环过程，优化摄像头的放置（位置、方向、变焦）仅使用2-D信息（第2.2节）。其次，每个摄像头都以精确的方式进行校准，使用麦克风阵列的2-D和3-D测量作为基准（第2.3节）。这个过程背后的想法是，如果我们能够跟踪每个摄像头图像平面上一个人的嘴，那么我们可以使用摄像头的校准参数重建嘴的3-D轨迹。这可以作为音频注释，前提是3-D轨迹是在麦克风阵列的基准上定义的。我们展示了3-D重建误差在一个非常可接受的范围内。

2.1 硬件

我们使用了3台摄像机和两个直径为10厘米的8麦克风阵列来自一个装置的会议室[6]。这两个麦克风阵列相距0.8米。这个选择背后的动机有三个:

- 使用两个麦克风阵列进行记录提供了用于3-D音频源定位和跟踪的测试案例，因为每个麦克风阵列可以用于提供每个音频源的(方位角、俯仰角)位置估计。
- 使用多台摄像机进行记录产生了许多有趣的真实视觉遮挡案例，从多个视角观看每个人。
- 至少需要两台摄像机来计算摄像头图像平面中物体的3-D坐标。使用三台摄像机可以重建3-D坐标。

步骤一：摄像头放置

本节描述了使用仅2-D信息来优化摄像头位置（位置、方向、变焦）的循环过程。我们使用了一个免费提供的多摄像头自校准（MultiC amSelfCal）软件10，“自校准”意味着校准点的3-D位置是未知的。MultiCamSelfCal仅使用每个摄像头在图像平面上的2D坐标，通过优化“2-D重投影误差”，联合为每个摄像头生成一组校准参数和校准点的3-D位置估计。对于每个摄像头，“2-D重投影误差”定义为记录的2-D点与它们的3-D位置估计在摄像头图像平面上的投影之间的像素间距，使用估计的摄像头校准参数。虽然我们只使用了具有最少数量摄像头的软件（三个），但获得的2-D重投影误差还可以接受：其上限被估计为小于0.17像素。

摄像头放置程序包括三个步骤的迭代过程：放置、记录和校准：

1. 根据以往经验在先前迭代中放置三个摄像头（位置、方向、变焦）。实际上，各摄像头应该提供尽可能不同的视图。
2. 与三个摄像头同步记录一组校准点，即每个摄像头图像平面上的2-D坐标。如10所述，在黑暗中挥动一根激光束即可。
3. 运行MultiCamSelfCal对校准点对三个摄像头进行校准，MultiCamSelfCal优化2-D重投影误差。
4. 为了尝试减少2-D重投影误差，循环到第1步。否则进入第5步。在实践中，2-D重投影误差在0.2像素以下是合理的。
5. 选择导致最小2-D重投影误差的摄像头放置。

多摄像头自校准一般被认为比使用已知3-D坐标的物体进行手动校准提供的精度要低。使用它的动机是易于使用：校准点可以快速通过激光束记录。因此，一次放置/记录/校准循环大约需要1小时30分钟。这个过程收敛到了图1所示的摄像头位置。有关详细信息，请阅读[10]中的文档，包括多摄像头自校准问题陈述。

第二步：相机校准

本节描述了每个相机的精确校准，假设相机的位置、方向、缩放固定。这是通过选择并优化每个相机的校准参数，在一个校准对象上完成的。对于校准对象的每个点，都已知在麦克风阵列参照下的真实3D坐标和在每个相机图像平面上的真实2D坐标。3D坐标是在现场使用测量卷尺获取的（估计测量误差不到0.005米）。图2中的十字标示出了3D校准点。这些点被分为两组：训练集（36个点）和测试集（39个点）。

特别需要提到模型选择问题，即我们选择如何模拟由每个相机光学系统产生的非线性失真。采用一个迭代过程，评估三个相机的校准参数的适用性，以“3D重建误差”为标准：从至少被2个相机看到的点的3D位置估计和其真实3D位置之间的欧几里得距离。相机校准过程详细步骤如下：

1. 模型选择：对于每个相机，基于之前迭代的经验，选择一组校准参数。
2. 模型训练：对每个相机，利用2train上的可用软件估计选定的校准参数。
3. 3D误差：对于2train中的每个点，计算真实3D坐标与使用训练的校准参数和每个相机图像平面上的2D坐标重建的3D坐标之间的欧几里得距离。
4. 评估：估计“训练”最大3D重建误差为μ+3σ，其中μ和σ分别为所有.train中所有点的3D误差的平均值和标准差。
5. 尝试降低最大3D重建误差，返回步骤1。否则，执行步骤6。
6. 选择使最大3D重建误差最小的校准参数集和其估计值。

这一过程的结果是每个相机的一组校准参数和其值。对于所有相机，最佳参数集是焦点中心、焦距、r'径向和切向失真系数。训练结束后，我们在未见的测试集2test上评估了3D误差。这个集合上的最大3D重建误差为0.012米。这个最大误差被认为是不错的，与张开嘴巴的直径相比（约为0.05米）。在线语料库

本节首先激励并描述了记录的各种序列，然后更详细地描述了已注释的序列。“序列”指：
-3个视频DIVX AVI文件（分辨率为288x360），每个相机一个，采样频率为25 Hz。还包括一个音频信号。
-从两个圆形8麦克风阵列录制的16个音频WAV文件，采样率为16 kHz。可能在讲话者佩戴的领夹上也有更多音频WAV文件，采样率为16 kHz。所有文件均以同步方式录制：视频文件在每个图像的上方行中嵌入了一个时间戳，音频文件始终从视频时间戳00:00:10,00开始。关于跨所有传感器实施的唯一时钟的硬件实现的完整详细信息可以在6]中找到。尽管只对8个序列进行了注释，但还有许多其他序列可用。整个语料库，以及注释文件、相机校准参数和其他文档，可在以下网址进行访问：[http://mm](http://mm/). [idiap.ch/lathoud/av16.3_v6。它在5天内录制，总共包括42个序列，序列持续时间从14秒到9分钟不等（总计1小时25分钟），共录制了12位不同的演员。尽管本文的作者也被录制了，但许多演员在音频和视频定位跟踪领域并没有任何特别的专业知识。](http://idiap.ch/lathoud/av16.3_v6。它在5天内录制，总共包括42个序列，序列持续时间从14秒到9分钟不等（总计1小时25分钟），共录制了12位不同的演员。尽管本文的作者也被录制了，但许多演员在音频和视频定位跟踪领域并没有任何特别的专业知识。)

3.1 动机
主要目标是研究多种定位/跟踪现象。一个非限制性的列表包括：

- 重叠语音。
  接近和远处的位置，小型和大型角度间隔。
  对象初始化。
  -不同数量的对象。
- 部分和完全遮挡。
- “自然”光照变化。
  因此，我们定义并记录了一系列包含多种测试案例的序列：从短，非常受限制，特定情况（例如视觉遮挡）到自然非约束背景中的自然口语和/或运动。每个序列都对音频、视频或音视频数据的分析至少有用。每个序列允许最多三个人。人类活动可以是静态的（例如，坐着的人）。动态的（例如，走路的人）或者跨人（例如，一些人坐着，一些人走路）和时间（例如，会议前后有人站着和走动）的混合。

3.2 内容
如上所述，在线语料库包括8个已注释序列以及许多未注释序列。选择这8个序列是为了进行初始注释工作。这种选择在有限数量序列进行注释与涵盖各种情况之间取得了平衡，以满足各个研究领域的兴趣。它构成了一组最小的序列，涵盖尽可能多的跨模态和说话者行为多样性。注释过程将在第4节中描述。每个序列的名称都是唯一的，表1提供了一个综合概述，接下来是对每个序列的更详细描述。

表1. 已注释的序列列表，标签意味着：[A]udio，[V]ideo，主要 [ov]erlapped演讲，至少有一个视觉[occ]lusion，[S]tatic演讲者，[D]ynamic演讲者，[U]nconstrained运动，[Mouth，[F]ace，[Head，speech/silence [seg]mentation.序列01-1p-0000 单个演讲者，在图1中阴影区域的每个16个位置静止讲话。演讲者面向麦克风阵列，此序列的目的是评估单个演讲者案例的音频源定位。序列11-1p-0100 一个演讲者，在说话时大部分时间在移动。演讲者运动的唯一约束是面向麦克风阵列。旨在测试困难运动案例上的音频、视频或音频-视频(AV)演讲者追踪。演讲者大部分时间都在讲话。序列15-1p-0100 一个移动的演讲者，在行走中交替说话和长时间沉默。此序列的目的是 1)显示音频追踪本身无法在沉默期间恢复不可预测的轨迹，2)为 AV 追踪提供一个初始测试案例。序列18-2p-0101 两名演讲者，一直讲话并一直面向麦克风阵列，慢慢地靠近对方然后慢慢地分开。目的是测试多源定位追踪和分离算法。序列24-2p-0111 两名移动演讲者，两次横穿视野并相互遮挡两次。两名演讲者大部分时间在讲话。旨在测试音频和视频遮挡效果。

表1. 注释序列列表，标签含义为：[A]音频，[V]视频，主要[ov]重叠讲话，至少一个视觉[occ]遮挡，[S]静态讲话者，[D]动态讲话者，[U]无约束运动，[口]口部，[脸]面部，[头] speech/silence [seg]mentati on.seq01-1p-0000 单个讲话者，在图1中覆盖的16个位置中的每个位置都静态讲话者，讲话者面向麦克风阵列，此序列的目的是评估单个讲话者情况下的音频源定位。seq11-1p-0100 一位讲话者，在讲话时大部分时间在运动。讲话者运动的唯一约束条件是面向麦克风阵列。动机是测试在困难的运动情况下的音频、视频或音视频（AV）说话跟踪。讲话者大部分时间在讲话。seq15-1p-0100 一个移动讲话者，边走动边交替讲话和长时间沉默。此序列的目的是1）表明单纯的音频跟踪无法从不可预测的沉默轨迹中恢复，2）为AV跟踪提供初步测试案例。seq18-2p-0101 两位讲话者，一直讲话且一直面向麦克风阵列，缓慢靠近然后分开。此序列的目的是测试多源定位跟踪和分离算法。
seq24-2p-0111 两位移动讲话者，两次穿过视野并遮挡对方两次。两位讲话者大部分时间在讲话。此序列的动机是测试音频和视频遮挡。seq37-3p-0001 三位静态讲话者。两位讲话者始终坐着，第三位站立。基本覆盖了五个位置，大部分时间有2或3位讲话者同时讲话。这个序列的目的是评估多源定位和波束形成算法。
seq40-3p-0111 三位讲话者，两位坐着一位站立，都在持续讲话且面向阵列，站立的讲话者在坐着的讲话者身后来回走动。动机是测试多源定位、跟踪和分离算法并突出音频和视频模态之间的互补性。
seq45-3p-1111 三位移动讲话者，进入并离开场景，持续讲话，多次遮挡对方。讲话者的运动是无约束的。这是一个极具挑战性的重叠讲话和视觉遮挡案例，其目标是突出音频和视频模态之间的互补性。

3.3 序列名称
已定义了一个系统编码规则，使得每个序列的名称 (1)是唯一的，并且 (2)包含对其内容的简洁描述。例如，“seg40-3p-0111”有三部分：

- “seg40”是该序列的唯一标识符。“3p”表示总共记录了3个不同的人，但不一定同时可见。
- “0111”则是四个二进制标志，快速概述了录像内容。从左到右：
  位 1：0 表示“非常受限”，1 表示“大部分无约束”（一般行为：尽管大多数录像都遵循某种情节，但有些包括非常强烈的约束，例如演讲者始终朝向麦克风阵列）。
  位 2：0 表示“静态运动”（例如大部分时间坐着），1 表示“动态运动”（例如持续运动）。
  位 3：0 表示“较小的遮挡”，1 表示“至少一个重要遮挡”，涉及至少一个阵列或摄像头：每当有人从其他人之前或之后走过时。
  位 4：0 表示“少量重叠”，1 表示“显著重叠”。 这涉及仅音频：指示是否存在演讲者和/或噪声源之间的重叠比例。

可以创建两种类型的注释：空间中的注释（例如扬声器轨迹）或时间中的注释（例如语音/静音分割）。注释的定义从根本上确定了用于评估定位和跟踪算法的性能指标。如何定义注释是有争议的。此外，我们注意到不同的模态（音频、视频）可能需要非常不同的注释（例如3-D口腔位置与2-D头部边界框）。第4.1节和4.2节报告了对AV16.3语料库进行的初始注释工作。第4.3、4.4和4.5节详细介绍了可用注释的一些应用示例。第4.6节讨论了注释的未来方向。
4.1 初始工作
只有两个具有静态扬声器的序列已经被完全注释：“seg01-1p-0000”和“seg37-3p-0001”。注释包括每个扬声器的3-D口腔位置和语音/静音分割。3-D口腔位置相对于麦克风阵列的参照系定义。

这个参考的原点位于两个麦克风阵列的中间。这些注解也可以在线访问。它已经被成功地用来评估最近的工作。此外，关于如何使用这些注解的简单示例可以在在线语料库中找到，如第4.3节中所述。

至于移动说话者和遮挡情况的序列，写了三个Matlab图形界面用于注解头部、口腔以及可选标记（彩色球）在人头上的位置：
BAI：球注解界面，用于标记一个带有颜色的球在人头上的位置，以椭圆形式呈现，可以标记遮挡情况，即当球不可见时。BAI包括一个简单的跟踪器来在手动测量之间进行插值。
HAI：头部注解界面，用于标记一个人头的位置，以矩形边界框形式呈现，可以标记部分或完全遮挡情况。
MAI：口腔注解界面，用于标记一个人口腔的位置作为一个点，也可以标记遮挡情况，即当口腔不可见时。
所有三个界面具有非常相似的功能，包括两个窗口：一个用于界面本身，另一个用于当前被注解的图像。HAI的快照示例可以在图3中看到。所有注解文件都是以ASCII格式存储的简单矩阵。所有三个界面均可在语料库本身内在线访问和文档化。我们已经使用它们从稀疏手动测量中产生连续的3D口腔位置注解，如第4.5节所述。

4.2当前状态
标注工作持续进行中，并且表2详细列出了截至2004年8月31日已经在网上提供的内容。
表2. 截至2004年8月31日在线提供的标注内容。“"表示连续标注，即对25 Hz视频的所有帧进行了标注。“S”表示稀疏标注，即标注的频率低于25 Hz（以括号中的数字给出）。

4.3示例1：音频来源定位评估
在线语料库包括一个完整的示例（Matlab文件），展示了单一来源定位，并与“seq01-1p-0000”序列的标注进行比较。它基于一种称为SRP-PHAT的参数化方法[3]。在网上可以找到运行示例所需的所有Matlab代码。比较结果显示，SRP-PHAT定位方法在方位角上提供了-5至+9度的精度。

4.4示例2：多目标视频跟踪
作为示例，使用三个独立的基于外观的粒子滤波器在“seq45-3p-1111”序列的200帧上应用的结果，仅使用一个摄像头进行展示，如图4和视频所示。该序列展示了三个人在房间中移动并交谈，包括多个对象遮挡的情况。每个跟踪器都是手动初始化的，并且使用了500个粒子。对象外观是通过RGB空间中的颜色分布[8]进行建模。在这个特定示例中，我们尚未进行任何性能评估。我们计划根据标注边界框与结果边界框的交集表面来定义精度和召回率。

5 结论
本文介绍了AV16.3语音定位和跟踪的数据集。AV16.3主要关注会议室数据的背景，通过3台摄像机、16个远距离麦克风和ADels同步获取数据。它针对多个研究领域：音频、视觉和音频-视觉说话人跟踪。为了提供音频标注，使用摄像头校准来生成“真实”的三维说话人嘴部位置，使用免费软件。据我们所知，这是第一次尝试为广泛的测试提供同步的音频-视觉数据，涉及各种测试案例，同时具有空间标注。AV163旨在作为系统性评估定位和跟踪算法在真实录音上的一步，未来的工作包括完成标注过程，可能还包括使用不同设置进行数据采集。

6 致谢
作者感谢欧盟通过AMI、M4、[HOARSE和IM2.SA](http://xn--hoarseim2-vw9o.sa/)、MUCATAR项目的支持。作者感谢所有在这个数据集中录制的参与者，Olivier Masson在物理设置方面的帮助，以及Mathew Magimai-Doss提供宝贵意见。